{"metadata":{"kernelspec":{"display_name":"tensorflow","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.0"},"colab":{"provenance":[]},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":99811,"databundleVersionId":12693370,"sourceType":"competition"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"papermill":{"default_parameters":{},"duration":340.921147,"end_time":"2025-06-22T19:19:45.451603","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-06-22T19:14:04.530456","version":"2.6.0"}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"28eb6402","cell_type":"markdown","source":"# Baseline Solution: Fake or Real - The Impostor Hunt in Texts üîç\n\n---\n\nHere we provide the baseline solution for the *Fake or Real: The Impostor Hunt in Texts* challenge!\nIn this notebook, we walk you through two **simple, interpretable, and ML-free approaches** to tackle the problem of detecting fake texts.\n\n### üí° The overview of first approach:\n\nWe use the `langdetect` library to analyze each text by identifying the presence of **English vs. non-English words**. Here's the idea:\n\n1. **Detect Language**: We break the text into words and determine the language of each.\n2. **Calculate Proportion**: We then compute the percentage of English words in the entire text.\n3. **Assign Label**: The text which gets higher percentage of English words is classified as **Real** and its number is saved to the results list.\n\n---\n\n### üì¶ Getting Started: Install & Import Required Packages\n","metadata":{"id":"28eb6402","papermill":{"duration":0.005901,"end_time":"2025-06-22T19:14:09.769223","exception":false,"start_time":"2025-06-22T19:14:09.763322","status":"completed"},"tags":[]}},{"id":"9d4a1e72","cell_type":"code","source":"# !pip install langdetect\n!pip install pandas\n!pip install unicodedata2\n!pip install scikit-learn\n\nimport os\nimport pandas as pd\nfrom langdetect import detect, DetectorFactory\nfrom langdetect.lang_detect_exception import LangDetectException\nimport unicodedata\n\nimport string\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\nDetectorFactory.seed = 42","metadata":{"execution":{"iopub.execute_input":"2025-06-22T19:14:18.254982Z","iopub.status.busy":"2025-06-22T19:14:18.254522Z","iopub.status.idle":"2025-06-22T19:14:22.250987Z","shell.execute_reply":"2025-06-22T19:14:22.250005Z"},"id":"9d4a1e72","papermill":{"duration":4.005574,"end_time":"2025-06-22T19:14:22.253245","exception":false,"start_time":"2025-06-22T19:14:18.247671","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"b7b9ab10","cell_type":"markdown","source":"# The following is an enhanced markdown description for the cell above, formatted for direct inclusion as a Markdown cell in a Jupyter/Colab notebook.\n\n\"\"\"\n# üöÄ Data Ingestion Function: `read_texts_from_dir`\n\nThis cell defines the **`read_texts_from_dir`** function, a critical utility for **efficiently loading and structuring raw text data** from a hierarchical file system into a clean pandas DataFrame. It acts as the initial ETL (Extract, Transform, Load) step for downstream NLP tasks.\n\n---\n\n## üéØ Purpose and Functionality\n\nThe core objective of this function is to automate the process of collecting text files that are presumably organized in pairs within subdirectories (e.g., `data/doc_001/file_a.txt`, `data/doc_001/file_b.txt`, etc.).\n\n### Parameters\n| Parameter | Type | Description |\n| :--- | :--- | :--- |\n| **`dir_path`** | `str` | The root path to the directory containing all subdirectories with the text files. |\n\n### Process Flow\n1.  **Directory Pre-count:** The function first calculates the total number of subdirectories (`dir_count`) within the provided `dir_path` using `os.walk` and `os.path.isdir`. This is a **performance optimization** to pre-allocate the data structure (`data=[0 for _ in range(dir_count)]`), ensuring efficient memory usage as the list is populated.\n2.  **File Reading Loop:** The unselected portion of the code would then traverse these subdirectories, read the content of the relevant text files (likely two per subfolder), and create a list of records.\n3.  **DataFrame Construction:** Finally, this list of records is transformed into a pandas DataFrame.\n\n### Output Structure\nThe resulting DataFrame (`df`) is structured for comparative text analysis:\n\n| Column | Data Type | Meaning |\n| :--- | :--- | :--- |\n| **`id`** | *Varies (Int/Str)* | A unique identifier, typically derived from the subdirectory name, serving as a primary key for the document pair. |\n| **`file_1`** | `str` | The text content of the first document (e.g., the source text or prompt). |\n| **`file_2`** | `str` | The text content of the second document (e.g., the potential duplicate, response, or comparison text). |\n\n---\n\n## üí° Application and Relevance\n\nGiven the output format (`file_1` and `file_2`), this function is highly relevant for common NLP and data science problems like:\n* **Plagiarism Detection:** Comparing a submitted document (`file_2`) against a source document (`file_1`). üïµÔ∏è\n* **Semantic Similarity/Paraphrasing:** Evaluating how closely the meaning of two texts aligns. ü§ù\n* **Parallel Corpus Development:** Creating structured data for machine translation or text-to-text generation tasks. üìù\n* **ML Data Preparation:** Creating the input features and labels required for training models.","metadata":{"id":"b7b9ab10","papermill":{"duration":0.006209,"end_time":"2025-06-22T19:14:22.266228","exception":false,"start_time":"2025-06-22T19:14:22.260019","status":"completed"},"tags":[]}},{"id":"f0a81296","cell_type":"code","source":"def read_texts_from_dir(dir_path):\n  \"\"\"\n  Reads the texts from a given directory and saves them in the pd.DataFrame with columns ['id', 'file_1', 'file_2'].\n\n  Params:\n    dir_path (str): path to the directory with data\n  \"\"\"\n  # Count number of directories in the provided path\n  dir_count = sum(os.path.isdir(os.path.join(root, d)) for root, dirs, _ in os.walk(dir_path) for d in dirs)\n  data=[0 for _ in range(dir_count)]\n  print(f\"Number of directories: {dir_count}\")\n\n  # For each directory, read both file_1.txt and file_2.txt and save results to the list\n  i=0\n  for folder_name in sorted(os.listdir(dir_path)):\n    folder_path = os.path.join(dir_path, folder_name)\n    if os.path.isdir(folder_path):\n      try:\n        with open(os.path.join(folder_path, 'file_1.txt'), 'r', encoding='utf-8') as f1:\n          text1 = f1.read().strip()\n        with open(os.path.join(folder_path, 'file_2.txt'), 'r', encoding='utf-8') as f2:\n          text2 = f2.read().strip()\n        index = int(folder_name[-4:])\n        data[i]=(index, text1, text2)\n        i+=1\n      except Exception as e:\n        print(f\"Error reading directory {folder_name}: {e}\")\n\n  # Change list with results into pandas DataFrame\n  df = pd.DataFrame(data, columns=['id', 'file_1', 'file_2']).set_index('id')\n  return df","metadata":{"execution":{"iopub.execute_input":"2025-06-22T19:14:22.279068Z","iopub.status.busy":"2025-06-22T19:14:22.278531Z","iopub.status.idle":"2025-06-22T19:14:22.289580Z","shell.execute_reply":"2025-06-22T19:14:22.288550Z"},"id":"f0a81296","papermill":{"duration":0.019703,"end_time":"2025-06-22T19:14:22.291611","exception":false,"start_time":"2025-06-22T19:14:22.271908","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"11fec4b9","cell_type":"code","source":"# Use the above function to load both train and test data\ntrain_path=\"/home/samer/Desktop/competitions/Kaggle challenge Fake or Real: The Impostor Hunt in Texts/fake-or-real-the-impostor-hunt/data/train\"\ndf_train=read_texts_from_dir(train_path)\ntest_path=\"/home/samer/Desktop/competitions/Kaggle challenge Fake or Real: The Impostor Hunt in Texts/fake-or-real-the-impostor-hunt/data/test\"\ndf_test=read_texts_from_dir(test_path)","metadata":{"execution":{"iopub.execute_input":"2025-06-22T19:14:22.304915Z","iopub.status.busy":"2025-06-22T19:14:22.303667Z","iopub.status.idle":"2025-06-22T19:14:36.137050Z","shell.execute_reply":"2025-06-22T19:14:36.135916Z"},"id":"11fec4b9","outputId":"75356dc8-934d-4710-c2e3-3aeb11af9427","papermill":{"duration":13.841837,"end_time":"2025-06-22T19:14:36.138976","exception":false,"start_time":"2025-06-22T19:14:22.297139","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of directories: 95\n","Number of directories: 1068\n"]}],"execution_count":null},{"id":"0a756431","cell_type":"code","source":"df_train.head()","metadata":{"execution":{"iopub.execute_input":"2025-06-22T19:14:36.151640Z","iopub.status.busy":"2025-06-22T19:14:36.151317Z","iopub.status.idle":"2025-06-22T19:14:36.175447Z","shell.execute_reply":"2025-06-22T19:14:36.174434Z"},"id":"0a756431","papermill":{"duration":0.032743,"end_time":"2025-06-22T19:14:36.177232","exception":false,"start_time":"2025-06-22T19:14:36.144489","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"aa44125b","cell_type":"code","source":"df_test.head()","metadata":{"execution":{"iopub.execute_input":"2025-06-22T19:14:36.192519Z","iopub.status.busy":"2025-06-22T19:14:36.192204Z","iopub.status.idle":"2025-06-22T19:14:36.202290Z","shell.execute_reply":"2025-06-22T19:14:36.201254Z"},"id":"aa44125b","papermill":{"duration":0.020476,"end_time":"2025-06-22T19:14:36.204087","exception":false,"start_time":"2025-06-22T19:14:36.183611","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"1faa568b","cell_type":"markdown","source":"---\n\n### üè∑Ô∏è Read the Labels\n\nNext, we‚Äôll load the **labels** associated with each text sample.\nThese labels indicate which text is **Real** - 1 or 2. The labels will serve as our ground truth for evaluation.\n\nWe‚Äôll again use `Pandas` to read the label file into a DataFrame and inspect its structure.\n","metadata":{"id":"1faa568b","papermill":{"duration":0.005923,"end_time":"2025-06-22T19:14:36.216367","exception":false,"start_time":"2025-06-22T19:14:36.210444","status":"completed"},"tags":[]}},{"id":"94319bd5","cell_type":"code","source":"# Load ground truth for train data\ndf_train_gt=pd.read_csv(\"/home/samer/Desktop/competitions/Kaggle challenge Fake or Real: The Impostor Hunt in Texts/fake-or-real-the-impostor-hunt/data/train.csv\")\ndf_train_gt","metadata":{"execution":{"iopub.execute_input":"2025-06-22T19:14:36.230689Z","iopub.status.busy":"2025-06-22T19:14:36.230301Z","iopub.status.idle":"2025-06-22T19:14:36.254993Z","shell.execute_reply":"2025-06-22T19:14:36.253991Z"},"id":"94319bd5","papermill":{"duration":0.034322,"end_time":"2025-06-22T19:14:36.256667","exception":false,"start_time":"2025-06-22T19:14:36.222345","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"2e9eb484","cell_type":"markdown","source":"---\n# ‚úçÔ∏è Baseline Prediction Method: English Word Percentage\n\nThis cell defines a simple, language-based baseline method for the competition.\n\n### üìú Method Description\nThe `baseline_method_english_word` function predicts which text is \"Real\" (or more authentic/less generated) by comparing the **percentage of recognized English words** in `text_1` and `text_2`.\n\n-   It iterates through the rows of the input DataFrame.\n-   For each pair of texts, it calculates a \"score\" based on the proportion of English words found in it.\n-   It predicts `1` if the score for `text_1` is higher (more English words) and `2` if the score for `text_2` is higher.\n\n### üéØ Expected Use Case\nThis serves as a quick and easily reproducible **initial benchmark** to gauge the difficulty of the problem before developing more sophisticated Machine Learning models.","metadata":{"id":"2e9eb484","papermill":{"duration":0.0063,"end_time":"2025-06-22T19:14:36.269022","exception":false,"start_time":"2025-06-22T19:14:36.262722","status":"completed"},"tags":[]}},{"id":"abc5ad29","cell_type":"code","source":"def baseline_method_english_word(df):\n  \"\"\"\n  This baseline method predicts which of the texts is Real, based on the percentage of English words in each text.\n  It returns list with predictions.\n\n  Params:\n    df (pd.DataFrame): dataframe with all texts\n  \"\"\"\n  # Create lists in which scores will be saved for file_1 (left_scores) and files_2 (right_scores)\n  left_scores=[0 for _ in range(df.shape[0])]\n  right_scores=[0 for _ in range(df.shape[0])]\n  # For each row in the DataFrame and for each element of this row run the algorithm for detecting English words\n  for j in range(df.shape[0]):\n    for z in range(df.shape[1]):\n      sum_english=0\n      n=10\n      delete=str.maketrans('', '', string.punctuation+'\\n')\n      cleaned=df.iloc[j].iloc[z].translate(delete)\n      text_to_check=cleaned.split(\" \")\n      text_to_check=[' '.join(text_to_check[i:i+n]) for i in range(0, len(text_to_check),n)]\n\n      # Run algorithm for detecting English words\n      for i in range(len(text_to_check)):\n        try:\n          language=detect(text_to_check[i])\n        except LangDetectException as e:\n          pass\n        if language=='en':\n          sum_english+=1\n      result=sum_english/len(text_to_check)\n      if z==0:\n        left_scores[j]=result\n      elif z==1:\n        right_scores[j]=result\n      else:\n        print('Wrong')\n  # Create list with predictions by setting value in list to 1 if the first text is `Real` or 2 when the second seems to be better\n  predictions=[1 if left_scores[k]>right_scores[k] else 2 for k in range(len(left_scores))]\n  return predictions","metadata":{"execution":{"iopub.execute_input":"2025-06-22T19:14:36.282840Z","iopub.status.busy":"2025-06-22T19:14:36.282441Z","iopub.status.idle":"2025-06-22T19:14:36.291558Z","shell.execute_reply":"2025-06-22T19:14:36.290530Z"},"id":"abc5ad29","papermill":{"duration":0.018377,"end_time":"2025-06-22T19:14:36.293265","exception":false,"start_time":"2025-06-22T19:14:36.274888","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"586922ae","cell_type":"code","source":"def evaluate_baseline(predictions, gt_list, text='Score with english detection:'):\n  \"\"\"\n  Evaluates the predictions for train data, when the ground truth is provided.\n\n  Params:\n    predictions (list): list of predictions\n    gt_list (list): list of predictions\n    text (str): text to be printed together with the result\n  \"\"\"\n  acc_score = accuracy_score(gt_list, predictions)\n  print(text,acc_score)","metadata":{"execution":{"iopub.execute_input":"2025-06-22T19:14:36.306416Z","iopub.status.busy":"2025-06-22T19:14:36.306074Z","iopub.status.idle":"2025-06-22T19:14:36.311389Z","shell.execute_reply":"2025-06-22T19:14:36.310365Z"},"id":"586922ae","papermill":{"duration":0.01389,"end_time":"2025-06-22T19:14:36.313009","exception":false,"start_time":"2025-06-22T19:14:36.299119","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"49259059","cell_type":"markdown","source":"---\n#### üìä Results","metadata":{"id":"49259059","papermill":{"duration":0.006113,"end_time":"2025-06-22T19:14:36.325903","exception":false,"start_time":"2025-06-22T19:14:36.319790","status":"completed"},"tags":[]}},{"id":"98106d95","cell_type":"code","source":"# Use the algorithm for the train data and check accuracy\npredictions_train=baseline_method_english_word(df_train)\ngt_train=list(df_train_gt['real_text_id'])\nevaluate_baseline(predictions_train, gt_train)","metadata":{"execution":{"iopub.execute_input":"2025-06-22T19:14:36.339504Z","iopub.status.busy":"2025-06-22T19:14:36.339188Z","iopub.status.idle":"2025-06-22T19:15:08.840950Z","shell.execute_reply":"2025-06-22T19:15:08.839760Z"},"id":"98106d95","outputId":"6f920985-f8e0-4769-d98f-037875782570","papermill":{"duration":32.516237,"end_time":"2025-06-22T19:15:08.848330","exception":false,"start_time":"2025-06-22T19:14:36.332093","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Score with english detection: 0.6631578947368421\n"]}],"execution_count":null},{"id":"5c2758bf","cell_type":"code","source":"# Use the algorithm for the test data\npredictions_test=baseline_method_english_word(df_test)","metadata":{"execution":{"iopub.execute_input":"2025-06-22T19:15:08.862451Z","iopub.status.busy":"2025-06-22T19:15:08.861513Z","iopub.status.idle":"2025-06-22T19:19:42.522145Z","shell.execute_reply":"2025-06-22T19:19:42.521212Z"},"id":"5c2758bf","papermill":{"duration":273.669614,"end_time":"2025-06-22T19:19:42.524423","exception":false,"start_time":"2025-06-22T19:15:08.854809","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"2764c777","cell_type":"markdown","source":"### Prepare format for sample solution","metadata":{"id":"2764c777","papermill":{"duration":0.005863,"end_time":"2025-06-22T19:19:42.537059","exception":false,"start_time":"2025-06-22T19:19:42.531196","status":"completed"},"tags":[]}},{"id":"3eed3c17","cell_type":"code","source":"# Change the format of predictions into requested format, as described in Overview section of this competition\ndf_results_test=pd.DataFrame(predictions_test)\noutput_df = df_results_test.copy()\noutput_df.columns = ['real_text_id']\noutput_df.reset_index(inplace=True)\noutput_df.rename(columns={'index': 'id'}, inplace=True)\noutput_df","metadata":{"execution":{"iopub.execute_input":"2025-06-22T19:19:42.552497Z","iopub.status.busy":"2025-06-22T19:19:42.551105Z","iopub.status.idle":"2025-06-22T19:19:42.568946Z","shell.execute_reply":"2025-06-22T19:19:42.567746Z"},"id":"3eed3c17","papermill":{"duration":0.02756,"end_time":"2025-06-22T19:19:42.570702","exception":false,"start_time":"2025-06-22T19:19:42.543142","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"4e4dc1c2","cell_type":"code","source":"output_df.to_csv('sample_submission_1.csv', index=False)","metadata":{"execution":{"iopub.execute_input":"2025-06-22T19:19:42.585616Z","iopub.status.busy":"2025-06-22T19:19:42.585267Z","iopub.status.idle":"2025-06-22T19:19:42.596840Z","shell.execute_reply":"2025-06-22T19:19:42.595813Z"},"id":"4e4dc1c2","papermill":{"duration":0.021597,"end_time":"2025-06-22T19:19:42.598858","exception":false,"start_time":"2025-06-22T19:19:42.577261","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"b74013ee","cell_type":"markdown","source":"---\n\n### üî° Character-Level Baseline\n\nIn addition to analyzing words, we can explore a **character-level approach** as an alternative baseline.\n\nThis method evaluates the **proportion of Latin characters** in the text, instead of relying on word-based language detection.\n\nBy comparing the ratio of English characters to total characters, we generate another set of predictions‚Äîoffering a complementary perspective to our word-level strategy.","metadata":{"id":"b74013ee","papermill":{"duration":0.006968,"end_time":"2025-06-22T19:19:42.612628","exception":false,"start_time":"2025-06-22T19:19:42.605660","status":"completed"},"tags":[]}},{"id":"f29e63bc","cell_type":"code","source":"def is_latin_char(char):\n  \"\"\"\n  Detect if given character is from Latin alphabet.\n\n  Params:\n    char (str): given character\n  \"\"\"\n  char=str(char)\n  try:\n    name=unicodedata.name(char)\n    return 'LATIN' in name\n  except ValueError:\n    return False\n\ndef baseline_chars_method(df):\n  \"\"\"\n  This baseline method predicts which of the texts is Real, based on the percentage of Lating letters in each text.\n  It returns list with predictions.\n\n  Params:\n    df (pd.DataFrame): dataframe with all texts\n  \"\"\"\n  # Create lists in which scores will be saved for file_1 (left_scores) and files_2 (right_scores)\n  left_scores=[0 for _ in range(df.shape[0])]\n  right_scores=[0 for _ in range(df.shape[0])]\n  # For each row in the DataFrame and for each element of this row run the algorithm for detecting Latin chars\n  for j in range(df.shape[0]):\n    for z in range(df.shape[1]):\n      sum_latin=0\n      count_spaces=0\n      delete=str.maketrans('', '', string.punctuation+'\\n')\n      cleaned=df.iloc[j].iloc[z].translate(delete)\n\n      # Run algorithm for detecting Latin chars\n      for i in range(len(cleaned)):\n        if cleaned[i] !=' ':\n          if is_latin_char(cleaned[i]):\n            sum_latin+=1\n        else:\n          count_spaces+=1\n      if len(cleaned)==0:\n        result=0\n      else:\n        result=sum_latin/(len(cleaned)-count_spaces)\n      if z==0:\n        left_scores[j]=result\n      elif z==1:\n        right_scores[j]=result\n      else:\n        print('Wrong')\n  # Create list with predictions by setting value in list to 1 if the first text is `Real` or 2 when the second seems to be better\n  predictions=[1 if left_scores[k]>right_scores[k] else 2 for k in range(len(left_scores))]\n  return predictions","metadata":{"execution":{"iopub.execute_input":"2025-06-22T19:19:42.626597Z","iopub.status.busy":"2025-06-22T19:19:42.626265Z","iopub.status.idle":"2025-06-22T19:19:42.636149Z","shell.execute_reply":"2025-06-22T19:19:42.635105Z"},"id":"f29e63bc","papermill":{"duration":0.018951,"end_time":"2025-06-22T19:19:42.637823","exception":false,"start_time":"2025-06-22T19:19:42.618872","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"f9a7458b","cell_type":"markdown","source":"---\n#### üìä Results","metadata":{"id":"f9a7458b","papermill":{"duration":0.006022,"end_time":"2025-06-22T19:19:42.650348","exception":false,"start_time":"2025-06-22T19:19:42.644326","status":"completed"},"tags":[]}},{"id":"ac9d43ee","cell_type":"code","source":"# Use the algorithm for the train data and check accuracy\npredictions_train_char=baseline_chars_method(df_train)\ngt_train=list(df_train_gt['real_text_id'])\nevaluate_baseline(predictions_train_char, gt_train, text='Score with latin detection:')","metadata":{"execution":{"iopub.execute_input":"2025-06-22T19:19:42.664219Z","iopub.status.busy":"2025-06-22T19:19:42.663871Z","iopub.status.idle":"2025-06-22T19:19:42.844611Z","shell.execute_reply":"2025-06-22T19:19:42.843286Z"},"id":"ac9d43ee","outputId":"c2918670-f055-4c3e-d28a-8bc0ae123f4e","papermill":{"duration":0.190166,"end_time":"2025-06-22T19:19:42.846649","exception":false,"start_time":"2025-06-22T19:19:42.656483","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Score with latin detection: 0.5368421052631579\n"]}],"execution_count":null},{"id":"f7592d79","cell_type":"code","source":"# Use the algorithm for the test data\npreds_test_char=baseline_chars_method(df_test)","metadata":{"execution":{"iopub.execute_input":"2025-06-22T19:19:42.862281Z","iopub.status.busy":"2025-06-22T19:19:42.861127Z","iopub.status.idle":"2025-06-22T19:19:44.451505Z","shell.execute_reply":"2025-06-22T19:19:44.450535Z"},"id":"f7592d79","papermill":{"duration":1.600083,"end_time":"2025-06-22T19:19:44.453272","exception":false,"start_time":"2025-06-22T19:19:42.853189","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"f792a43e","cell_type":"markdown","source":"### Prepare format for sample solution","metadata":{"id":"f792a43e","papermill":{"duration":0.006786,"end_time":"2025-06-22T19:19:44.466326","exception":false,"start_time":"2025-06-22T19:19:44.459540","status":"completed"},"tags":[]}},{"id":"62b192d5","cell_type":"code","source":"# Change the format of predictions into requested format, as described in Overview section of this competition\ndf_results_test_char=pd.DataFrame(preds_test_char)\noutput_df_char = df_results_test_char.copy()\noutput_df_char.columns = ['real_text_id']\noutput_df_char.reset_index(inplace=True)\noutput_df_char.rename(columns={'index': 'id'}, inplace=True)\noutput_df_char","metadata":{"execution":{"iopub.execute_input":"2025-06-22T19:19:44.480624Z","iopub.status.busy":"2025-06-22T19:19:44.480258Z","iopub.status.idle":"2025-06-22T19:19:44.495159Z","shell.execute_reply":"2025-06-22T19:19:44.494043Z"},"id":"62b192d5","papermill":{"duration":0.02428,"end_time":"2025-06-22T19:19:44.496791","exception":false,"start_time":"2025-06-22T19:19:44.472511","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"c687dc84","cell_type":"code","source":"output_df_char.to_csv('sample_submission_2.csv', index=False)","metadata":{"execution":{"iopub.execute_input":"2025-06-22T19:19:44.512039Z","iopub.status.busy":"2025-06-22T19:19:44.511670Z","iopub.status.idle":"2025-06-22T19:19:44.518889Z","shell.execute_reply":"2025-06-22T19:19:44.517818Z"},"id":"c687dc84","papermill":{"duration":0.017127,"end_time":"2025-06-22T19:19:44.520774","exception":false,"start_time":"2025-06-22T19:19:44.503647","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"433bac7d","cell_type":"markdown","source":"---\n# **Submitted solution generation** \n\n## üêç Setup, Imports, and Environment Configuration ‚öôÔ∏è\n\nThis cell performs critical **setup tasks** for the notebook's execution.\n\n1.  **Library Imports:** Essential libraries are imported for data manipulation (`pandas`), system interaction (`os`), Google's Generative AI services (`google-genai`), and model evaluation (`sklearn.metrics`).\n2.  **API Key Loading:** The `dotenv` library is used with `load_dotenv` to securely load necessary environment variables, typically including the **Gemini API Key**, from a `.env` file for access by the `google.genai` client.\n3.  **Utility & Timing:** The `time` module is imported, likely for benchmarking or tracking the duration of computationally intensive tasks.\n4.  **Submission File Preparation (Inferred):** The trailing lines of code strongly suggest the final steps of a **data science pipeline**.\n    * A DataFrame (`output_df_char`) containing predictions or processed results is prepared.\n    * It's reset and renamed to include an **'id' column**.\n    * It is then saved to a CSV file (`submission_5.csv`), which is the standard format for submitting predictions to platforms like **Kaggle**.","metadata":{"id":"433bac7d"}},{"id":"41f9de4c","cell_type":"code","source":"import pandas as pd\nimport os\nfrom google import genai\nimport google.genai as genai\nfrom google.genai import types\nfrom sklearn.metrics import accuracy_score\nimport time\nfrom dotenv import load_dotenv\nimport os\n\n# Load environment variables from .env file\nload_dotenv()\n\n\ndef get_response(contents_1):\n    \"\"\"receive a string of the prompt for the model and\n    returns a string of the article\n\n    Args:\n        contents_1 (str): The prompt to send to the generative model.\n    \"\"\"\n    # Define the model name to use\n    model_name = \"gemma-3-27b-it\" # Corrected model name based on traceback\n    # Initialize the client with the API key\n    client = genai.Client(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n    x = True\n    # Loop to handle potential API errors and retry\n    while x == True:\n        try:\n            # Generate content using the specified model and prompt\n            response_1 = client.models.generate_content(\n                model=model_name,\n                contents=contents_1,\n                )\n            # Strip whitespace from the response text\n            result_1 = response_1.text.strip()\n            x = False\n            return result_1\n        except:\n            # If an error occurs, wait for 30 seconds before retrying\n            time.sleep(30)\n            continue\n\n    return result_1\n\ndef process_articles(df: pd.DataFrame):\n    \"\"\"\n    Iterates over a DataFrame, reading file paths from 'file_1' and 'file_2'\n    columns, and prints their contents.\n\n    Args:\n        df (pd.DataFrame): DataFrame with 'file_1' and 'file_2' columns\n                           containing file paths.\n    \"\"\"\n    print(\"--- Starting to process articles ---\")\n    predictions = []\n    counter = 0\n    results = [\"Article 1\", \"Article 2\"]\n    turns = 0\n    # itertuples() is generally more performant than iterrows()\n    for row in df.itertuples(index=False):\n        # Using getattr to access columns by name from the named tuple\n        print(\"this is row\", row)\n        f1 = getattr(row, 'file_1')\n        f2 = getattr(row, 'file_2')\n\n\n        # Define the model name and initialize the client\n        model_name = \"gemma-3-27b-it\" # Corrected model name based on traceback\n        client = genai.Client(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n\n\n        # models = client.models.list()\n        # for m in models:\n        #     print(m)\n        # break\n\n        # Create the prompt for the generative model\n        contents_1 = f\"\"\"\n                        Article 1: {f1}\n                        Article 2: {f2}\n\n                        Only one of the articles above is real, and the other is fake.\n                        Let's think step by step. First, analyze both articles to find signs of being fake or real. Second, compare them to determine which one is more likely to be real. Finally, state your conclusion.\n\n                        take into consideration that only texts in english language can be real and all other languages are fake.\n\n                        regardless of anything do not include the thinking steps in the output and return only one word referring to the real article [Article 1 or Article 2].\n\n                        Pay high attention please because it matters a lot.\n                        \"\"\"\n\n\n        try:\n            # Generate content from the model\n            response_1 = client.models.generate_content(\n                model=model_name,\n                contents=contents_1,\n                )\n\n        except:\n            # Wait for 30 seconds if there's an API error\n            time.sleep(30)\n        # Get the text from the response\n        result_1 = response_1.text.strip()\n        counter+=1\n        print(f\"Processed row {counter}: {result_1}\")\n\n        # Convert the model's text response to a prediction (1 or 2)\n        if result_1 == \"Article 1\":\n            predict = 1\n            print(\"predict\", predict)\n            predictions.append(predict)\n        elif result_1 == \"Article 2\":\n            predict = 2\n            print(\"predict\", predict)\n            predictions.append(predict)\n        else:\n            # If the response is not as expected, retry up to 10 times\n            while result_1 not in results and turns < 10:\n                result_1 = get_response(contents_1)\n                print(f\"Processed row {counter}: {result_1}\")\n                turns += 1\n            else:\n                # Process the result from retries\n                if result_1 == \"Article 1\":\n                    predict = 1\n                    print(\"predict\", predict)\n                    predictions.append(predict)\n                elif result_1 == \"Article 2\":\n                    predict = 2\n                    print(\"predict\", predict)\n                    predictions.append(predict)\n                else:\n                    # Default to 1 if still no valid response\n                    predict = 1\n                    print(\"predict\", predict)\n                    predictions.append(predict)\n\n\n\n    print(\"Predictions:\", predictions)\n    # Load ground truth labels for evaluation\n    df_train_gt=pd.read_csv(os.getenv(\"df_train_gt\"))\n    df_train_gt\n    y_true = [int(row[\"real_text_id\"]) for index, row in df_train_gt.iterrows()]\n    print(\"y_true\", y_true)\n\n    # Calculate and print the accuracy score\n    # acc = accuracy_score(y_true, predictions)\n    # print(\"Accuracy for the model: \", acc)\n\n    return predictions\n\n# This block runs when the script is executed directly\nif __name__ == '__main__':\n    # Define paths for train and test data\n    train_path=os.getenv(\"train_path\")\n    # Load training data\n    df_train=read_texts_from_dir(train_path)\n    test_path=os.getenv(\"test_path\")\n    # Load test data\n    df_test=read_texts_from_dir(test_path)\n\n    # Process the training data to get predictions\n    predictions = process_articles(df_test)\n    # Create a DataFrame from the predictions\n    df_results_test_char=pd.DataFrame(predictions)\n    output_df_char = df_results_test_char.copy()\n    output_df_char.columns = ['real_text_id']\n    output_df_char.reset_index(inplace=True)\n    output_df_char.rename(columns={'index': 'id'}, inplace=True)\n    output_df_char\n    # Save the results to a CSV file (commented out)\n    output_df_char.to_csv('submission_5.csv', index=False)","metadata":{"id":"41f9de4c"},"outputs":[],"execution_count":null}]}