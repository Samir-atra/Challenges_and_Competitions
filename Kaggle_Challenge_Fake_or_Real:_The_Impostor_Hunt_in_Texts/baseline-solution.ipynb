{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28eb6402",
   "metadata": {
    "id": "2-uxElrLnwNy",
    "papermill": {
     "duration": 0.005901,
     "end_time": "2025-06-22T19:14:09.769223",
     "exception": false,
     "start_time": "2025-06-22T19:14:09.763322",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Baseline Solution: Fake or Real - The Impostor Hunt in Texts üîç\n",
    "\n",
    "---\n",
    "\n",
    "Here we provide the baseline solution for the *Fake or Real: The Impostor Hunt in Texts* challenge!\n",
    "In this notebook, we walk you through two **simple, interpretable, and ML-free approaches** to tackle the problem of detecting fake texts.\n",
    "\n",
    "### üí° The overview of first approach:\n",
    "\n",
    "We use the `langdetect` library to analyze each text by identifying the presence of **English vs. non-English words**. Here's the idea:\n",
    "\n",
    "1. **Detect Language**: We break the text into words and determine the language of each.\n",
    "2. **Calculate Proportion**: We then compute the percentage of English words in the entire text.\n",
    "3. **Assign Label**: The text which gets higher percentage of English words is classified as **Real** and its number is saved to the results list.\n",
    "\n",
    "---\n",
    "\n",
    "### üì¶ Getting Started: Install & Import Required Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872ab685",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T19:14:09.780522Z",
     "iopub.status.busy": "2025-06-22T19:14:09.780145Z",
     "iopub.status.idle": "2025-06-22T19:14:18.239898Z",
     "shell.execute_reply": "2025-06-22T19:14:18.238668Z"
    },
    "id": "LCyK5ruYWdAp",
    "outputId": "6e27c50a-b46e-49c3-d7cb-1a7c25ad6af3",
    "papermill": {
     "duration": 8.467807,
     "end_time": "2025-06-22T19:14:18.241966",
     "exception": false,
     "start_time": "2025-06-22T19:14:09.774159",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /home/samer/anaconda3/envs/tensorflow/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /home/samer/anaconda3/envs/tensorflow/lib/python3.12/site-packages (from pandas) (2.2.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/samer/anaconda3/envs/tensorflow/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/samer/anaconda3/envs/tensorflow/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/samer/anaconda3/envs/tensorflow/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/samer/anaconda3/envs/tensorflow/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Collecting unicodedata2\n",
      "  Downloading unicodedata2-16.0.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\n",
      "Downloading unicodedata2-16.0.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (526 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m526.8/526.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: unicodedata2\n",
      "Successfully installed unicodedata2-16.0.0\n",
      "Requirement already satisfied: scikit-learn in /home/samer/anaconda3/envs/tensorflow/lib/python3.12/site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home/samer/anaconda3/envs/tensorflow/lib/python3.12/site-packages (from scikit-learn) (2.2.6)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/samer/anaconda3/envs/tensorflow/lib/python3.12/site-packages (from scikit-learn) (1.15.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/samer/anaconda3/envs/tensorflow/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/samer/anaconda3/envs/tensorflow/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: mediapipe in /home/samer/anaconda3/envs/tensorflow/lib/python3.12/site-packages (0.10.14)\n",
      "Requirement already satisfied: absl-py in /home/samer/anaconda3/envs/tensorflow/lib/python3.12/site-packages (from mediapipe) (1.4.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in /home/samer/anaconda3/envs/tensorflow/lib/python3.12/site-packages (from mediapipe) (24.2.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /home/samer/anaconda3/envs/tensorflow/lib/python3.12/site-packages (from mediapipe) (24.3.25)\n",
      "Requirement already satisfied: jax in /home/samer/anaconda3/envs/tensorflow/lib/python3.12/site-packages (from mediapipe) (0.6.2)\n",
      "Requirement already satisfied: jaxlib in /home/samer/anaconda3/envs/tensorflow/lib/python3.12/site-packages (from mediapipe) (0.6.2)\n",
      "Requirement already satisfied: matplotlib in /home/samer/anaconda3/envs/tensorflow/lib/python3.12/site-packages (from mediapipe) (3.9.3)\n",
      "Requirement already satisfied: numpy in /home/samer/anaconda3/envs/tensorflow/lib/python3.12/site-packages (from mediapipe) (2.2.6)\n",
      "Requirement already satisfied: opencv-contrib-python in /home/samer/anaconda3/envs/tensorflow/lib/python3.12/site-packages (from mediapipe) (4.12.0.88)\n",
      "Collecting protobuf<5,>=4.25.3 (from mediapipe)\n",
      "  Using cached protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in /home/samer/anaconda3/envs/tensorflow/lib/python3.12/site-packages (from mediapipe) (0.5.1)\n",
      "Requirement already satisfied: CFFI>=1.0 in /home/samer/anaconda3/envs/tensorflow/lib/python3.12/site-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /home/samer/anaconda3/envs/tensorflow/lib/python3.12/site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
      "Requirement already satisfied: ml_dtypes>=0.5.0 in /home/samer/anaconda3/envs/tensorflow/lib/python3.12/site-packages (from jax->mediapipe) (0.5.1)\n",
      "Requirement already satisfied: opt_einsum in /home/samer/anaconda3/envs/tensorflow/lib/python3.12/site-packages (from jax->mediapipe) (3.3.0)\n",
      "Requirement already satisfied: scipy>=1.12 in /home/samer/anaconda3/envs/tensorflow/lib/python3.12/site-packages (from jax->mediapipe) (1.15.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/samer/anaconda3/envs/tensorflow/lib/python3.12/site-packages (from matplotlib->mediapipe) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/samer/anaconda3/envs/tensorflow/lib/python3.12/site-packages (from matplotlib->mediapipe) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/samer/anaconda3/envs/tensorflow/lib/python3.12/site-packages (from matplotlib->mediapipe) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/samer/anaconda3/envs/tensorflow/lib/python3.12/site-packages (from matplotlib->mediapipe) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/samer/anaconda3/envs/tensorflow/lib/python3.12/site-packages (from matplotlib->mediapipe) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /home/samer/anaconda3/envs/tensorflow/lib/python3.12/site-packages (from matplotlib->mediapipe) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/samer/anaconda3/envs/tensorflow/lib/python3.12/site-packages (from matplotlib->mediapipe) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/samer/anaconda3/envs/tensorflow/lib/python3.12/site-packages (from matplotlib->mediapipe) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /home/samer/anaconda3/envs/tensorflow/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.17.0)\n",
      "Using cached protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "Installing collected packages: protobuf\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.3\n",
      "    Uninstalling protobuf-3.20.3:\n",
      "      Successfully uninstalled protobuf-3.20.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "xmanager 0.7.1 requires alembic==1.4.3, but you have alembic 1.16.4 which is incompatible.\n",
      "xmanager 0.7.1 requires sqlalchemy==1.2.19, but you have sqlalchemy 2.0.41 which is incompatible.\n",
      "tf-models-official 2.18.0 requires tensorflow~=2.18.0, but you have tensorflow 2.19.0 which is incompatible.\n",
      "opentelemetry-proto 1.36.0 requires protobuf<7.0,>=5.0, but you have protobuf 4.25.8 which is incompatible.\n",
      "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.2.6 which is incompatible.\n",
      "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\n",
      "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.19.0 which is incompatible.\n",
      "tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.19.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed protobuf-4.25.8\n"
     ]
    }
   ],
   "source": [
    "# !pip install langdetect\n",
    "!pip install pandas\n",
    "!pip install unicodedata2\n",
    "!pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d4a1e72",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T19:14:18.254982Z",
     "iopub.status.busy": "2025-06-22T19:14:18.254522Z",
     "iopub.status.idle": "2025-06-22T19:14:22.250987Z",
     "shell.execute_reply": "2025-06-22T19:14:22.250005Z"
    },
    "id": "ImkKXISLXN6V",
    "papermill": {
     "duration": 4.005574,
     "end_time": "2025-06-22T19:14:22.253245",
     "exception": false,
     "start_time": "2025-06-22T19:14:18.247671",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langdetect'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangdetect\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m detect, DetectorFactory\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangdetect\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlang_detect_exception\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LangDetectException\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01municodedata\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langdetect'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from langdetect import detect, DetectorFactory\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "import unicodedata\n",
    "\n",
    "import string\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "DetectorFactory.seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b9ab10",
   "metadata": {
    "id": "K9Sj8cJcYooa",
    "papermill": {
     "duration": 0.006209,
     "end_time": "2025-06-22T19:14:22.266228",
     "exception": false,
     "start_time": "2025-06-22T19:14:22.260019",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "### üìÑ Load the Data\n",
    "\n",
    "Now, let's load the data into memory for exploration and processing.\n",
    "\n",
    "We'll use `Pandas` to read the file into a DataFrame, which allows for easy data manipulation and analysis throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0a81296",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T19:14:22.279068Z",
     "iopub.status.busy": "2025-06-22T19:14:22.278531Z",
     "iopub.status.idle": "2025-06-22T19:14:22.289580Z",
     "shell.execute_reply": "2025-06-22T19:14:22.288550Z"
    },
    "id": "l4NUJlOdjlCU",
    "papermill": {
     "duration": 0.019703,
     "end_time": "2025-06-22T19:14:22.291611",
     "exception": false,
     "start_time": "2025-06-22T19:14:22.271908",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_texts_from_dir(dir_path):\n",
    "  \"\"\"\n",
    "  Reads the texts from a given directory and saves them in the pd.DataFrame with columns ['id', 'file_1', 'file_2'].\n",
    "\n",
    "  Params:\n",
    "    dir_path (str): path to the directory with data\n",
    "  \"\"\"\n",
    "  # Count number of directories in the provided path\n",
    "  dir_count = sum(os.path.isdir(os.path.join(root, d)) for root, dirs, _ in os.walk(dir_path) for d in dirs)\n",
    "  data=[0 for _ in range(dir_count)]\n",
    "  print(f\"Number of directories: {dir_count}\")\n",
    "\n",
    "  # For each directory, read both file_1.txt and file_2.txt and save results to the list\n",
    "  i=0\n",
    "  for folder_name in sorted(os.listdir(dir_path)):\n",
    "    folder_path = os.path.join(dir_path, folder_name)\n",
    "    if os.path.isdir(folder_path):\n",
    "      try:\n",
    "        with open(os.path.join(folder_path, 'file_1.txt'), 'r', encoding='utf-8') as f1:\n",
    "          text1 = f1.read().strip()\n",
    "        with open(os.path.join(folder_path, 'file_2.txt'), 'r', encoding='utf-8') as f2:\n",
    "          text2 = f2.read().strip()\n",
    "        index = int(folder_name[-4:])\n",
    "        data[i]=(index, text1, text2)\n",
    "        i+=1\n",
    "      except Exception as e:\n",
    "        print(f\"Error reading directory {folder_name}: {e}\")\n",
    "\n",
    "  # Change list with results into pandas DataFrame\n",
    "  df = pd.DataFrame(data, columns=['id', 'file_1', 'file_2']).set_index('id')\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11fec4b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T19:14:22.304915Z",
     "iopub.status.busy": "2025-06-22T19:14:22.303667Z",
     "iopub.status.idle": "2025-06-22T19:14:36.137050Z",
     "shell.execute_reply": "2025-06-22T19:14:36.135916Z"
    },
    "id": "9zQl8N5NjsL_",
    "outputId": "75356dc8-934d-4710-c2e3-3aeb11af9427",
    "papermill": {
     "duration": 13.841837,
     "end_time": "2025-06-22T19:14:36.138976",
     "exception": false,
     "start_time": "2025-06-22T19:14:22.297139",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of directories: 95\n",
      "Number of directories: 1068\n"
     ]
    }
   ],
   "source": [
    "# Use the above function to load both train and test data\n",
    "train_path=\"/home/samer/Desktop/competitions/Kaggle challenge Fake or Real: The Impostor Hunt in Texts/fake-or-real-the-impostor-hunt/data/train\"\n",
    "df_train=read_texts_from_dir(train_path)\n",
    "test_path=\"/home/samer/Desktop/competitions/Kaggle challenge Fake or Real: The Impostor Hunt in Texts/fake-or-real-the-impostor-hunt/data/test\"\n",
    "df_test=read_texts_from_dir(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a756431",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T19:14:36.151640Z",
     "iopub.status.busy": "2025-06-22T19:14:36.151317Z",
     "iopub.status.idle": "2025-06-22T19:14:36.175447Z",
     "shell.execute_reply": "2025-06-22T19:14:36.174434Z"
    },
    "id": "ZfXeI_2kQwOY",
    "outputId": "56436a79-d662-444b-b568-292ea9d4d963",
    "papermill": {
     "duration": 0.032743,
     "end_time": "2025-06-22T19:14:36.177232",
     "exception": false,
     "start_time": "2025-06-22T19:14:36.144489",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_1</th>\n",
       "      <th>file_2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The VIRSA (Visible Infrared Survey Telescope A...</td>\n",
       "      <td>The China relay network has released a signifi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>China\\nThe goal of this project involves achie...</td>\n",
       "      <td>The project aims to achieve an accuracy level ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Scientists can learn about how galaxies form a...</td>\n",
       "      <td>Dinosaur eggshells offer clues about what dino...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>China\\nThe study suggests that multiple star s...</td>\n",
       "      <td>The importance for understanding how stars evo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dinosaur Rex was excited about his new toy set...</td>\n",
       "      <td>Analyzing how fast stars rotate within a galax...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               file_1  \\\n",
       "id                                                      \n",
       "0   The VIRSA (Visible Infrared Survey Telescope A...   \n",
       "1   China\\nThe goal of this project involves achie...   \n",
       "2   Scientists can learn about how galaxies form a...   \n",
       "3   China\\nThe study suggests that multiple star s...   \n",
       "4   Dinosaur Rex was excited about his new toy set...   \n",
       "\n",
       "                                               file_2  \n",
       "id                                                     \n",
       "0   The China relay network has released a signifi...  \n",
       "1   The project aims to achieve an accuracy level ...  \n",
       "2   Dinosaur eggshells offer clues about what dino...  \n",
       "3   The importance for understanding how stars evo...  \n",
       "4   Analyzing how fast stars rotate within a galax...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa44125b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T19:14:36.192519Z",
     "iopub.status.busy": "2025-06-22T19:14:36.192204Z",
     "iopub.status.idle": "2025-06-22T19:14:36.202290Z",
     "shell.execute_reply": "2025-06-22T19:14:36.201254Z"
    },
    "id": "Bm1krJ-2kUVd",
    "outputId": "4b4a6131-05dd-4fa4-dbf1-0364009b288b",
    "papermill": {
     "duration": 0.020476,
     "end_time": "2025-06-22T19:14:36.204087",
     "exception": false,
     "start_time": "2025-06-22T19:14:36.183611",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_1</th>\n",
       "      <th>file_2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"Music\" Music music music Music music Music mu...</td>\n",
       "      <td>Since its launch on Paranal observatory's Very...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>underground exploration on SN's birth has prov...</td>\n",
       "      <td>SN 1987A provides valuable insights as newer o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This research aimed to understand how star sha...</td>\n",
       "      <td>ChromeDriver music player\\nThis study focused ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Using OmegaCAM's wide field capabilities spann...</td>\n",
       "      <td>greek translation :\\nvazhi (megaCAM), territor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AssemblyCulture AssemblyCulture AssemblyCultur...</td>\n",
       "      <td>XClass is software tool that helps astronomers...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               file_1  \\\n",
       "id                                                      \n",
       "0   \"Music\" Music music music Music music Music mu...   \n",
       "1   underground exploration on SN's birth has prov...   \n",
       "2   This research aimed to understand how star sha...   \n",
       "3   Using OmegaCAM's wide field capabilities spann...   \n",
       "4   AssemblyCulture AssemblyCulture AssemblyCultur...   \n",
       "\n",
       "                                               file_2  \n",
       "id                                                     \n",
       "0   Since its launch on Paranal observatory's Very...  \n",
       "1   SN 1987A provides valuable insights as newer o...  \n",
       "2   ChromeDriver music player\\nThis study focused ...  \n",
       "3   greek translation :\\nvazhi (megaCAM), territor...  \n",
       "4   XClass is software tool that helps astronomers...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1faa568b",
   "metadata": {
    "id": "--YetA0tY2GR",
    "papermill": {
     "duration": 0.005923,
     "end_time": "2025-06-22T19:14:36.216367",
     "exception": false,
     "start_time": "2025-06-22T19:14:36.210444",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "### üè∑Ô∏è Read the Labels\n",
    "\n",
    "Next, we‚Äôll load the **labels** associated with each text sample.\n",
    "These labels indicate which text is **Real** - 1 or 2. The labels will serve as our ground truth for evaluation.\n",
    "\n",
    "We‚Äôll again use `Pandas` to read the label file into a DataFrame and inspect its structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94319bd5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T19:14:36.230689Z",
     "iopub.status.busy": "2025-06-22T19:14:36.230301Z",
     "iopub.status.idle": "2025-06-22T19:14:36.254993Z",
     "shell.execute_reply": "2025-06-22T19:14:36.253991Z"
    },
    "id": "GH2JbNTTVQoI",
    "outputId": "29ec5bb1-1c7f-4e68-da23-d1aeb26f630a",
    "papermill": {
     "duration": 0.034322,
     "end_time": "2025-06-22T19:14:36.256667",
     "exception": false,
     "start_time": "2025-06-22T19:14:36.222345",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>real_text_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>90</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>91</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>92</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>93</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>94</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>95 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  real_text_id\n",
       "0    0             1\n",
       "1    1             2\n",
       "2    2             1\n",
       "3    3             2\n",
       "4    4             2\n",
       "..  ..           ...\n",
       "90  90             2\n",
       "91  91             1\n",
       "92  92             2\n",
       "93  93             2\n",
       "94  94             1\n",
       "\n",
       "[95 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load ground truth for train data\n",
    "df_train_gt=pd.read_csv(\"/home/samer/Desktop/competitions/Kaggle challenge Fake or Real: The Impostor Hunt in Texts/fake-or-real-the-impostor-hunt/data/train.csv\")\n",
    "df_train_gt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9eb484",
   "metadata": {
    "id": "C-03yeziJEbd",
    "papermill": {
     "duration": 0.0063,
     "end_time": "2025-06-22T19:14:36.269022",
     "exception": false,
     "start_time": "2025-06-22T19:14:36.262722",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "### üß™ Baseline solution with English words detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abc5ad29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T19:14:36.282840Z",
     "iopub.status.busy": "2025-06-22T19:14:36.282441Z",
     "iopub.status.idle": "2025-06-22T19:14:36.291558Z",
     "shell.execute_reply": "2025-06-22T19:14:36.290530Z"
    },
    "id": "JpDNYy3upspd",
    "papermill": {
     "duration": 0.018377,
     "end_time": "2025-06-22T19:14:36.293265",
     "exception": false,
     "start_time": "2025-06-22T19:14:36.274888",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def baseline_method_english_word(df):\n",
    "  \"\"\"\n",
    "  This baseline method predicts which of the texts is Real, based on the percentage of English words in each text.\n",
    "  It returns list with predictions.\n",
    "\n",
    "  Params:\n",
    "    df (pd.DataFrame): dataframe with all texts\n",
    "  \"\"\"\n",
    "  # Create lists in which scores will be saved for file_1 (left_scores) and files_2 (right_scores)\n",
    "  left_scores=[0 for _ in range(df.shape[0])]\n",
    "  right_scores=[0 for _ in range(df.shape[0])]\n",
    "  # For each row in the DataFrame and for each element of this row run the algorithm for detecting English words\n",
    "  for j in range(df.shape[0]):\n",
    "    for z in range(df.shape[1]):\n",
    "      sum_english=0\n",
    "      n=10\n",
    "      delete=str.maketrans('', '', string.punctuation+'\\n')\n",
    "      cleaned=df.iloc[j].iloc[z].translate(delete)\n",
    "      text_to_check=cleaned.split(\" \")\n",
    "      text_to_check=[' '.join(text_to_check[i:i+n]) for i in range(0, len(text_to_check),n)]\n",
    "\n",
    "      # Run algorithm for detecting English words\n",
    "      for i in range(len(text_to_check)):\n",
    "        try:\n",
    "          language=detect(text_to_check[i])\n",
    "        except LangDetectException as e:\n",
    "          pass\n",
    "        if language=='en':\n",
    "          sum_english+=1\n",
    "      result=sum_english/len(text_to_check)\n",
    "      if z==0:\n",
    "        left_scores[j]=result\n",
    "      elif z==1:\n",
    "        right_scores[j]=result\n",
    "      else:\n",
    "        print('Wrong')\n",
    "  # Create list with predictions by setting value in list to 1 if the first text is `Real` or 2 when the second seems to be better\n",
    "  predictions=[1 if left_scores[k]>right_scores[k] else 2 for k in range(len(left_scores))]\n",
    "  return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "586922ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T19:14:36.306416Z",
     "iopub.status.busy": "2025-06-22T19:14:36.306074Z",
     "iopub.status.idle": "2025-06-22T19:14:36.311389Z",
     "shell.execute_reply": "2025-06-22T19:14:36.310365Z"
    },
    "id": "xAIxmZsWqBIV",
    "papermill": {
     "duration": 0.01389,
     "end_time": "2025-06-22T19:14:36.313009",
     "exception": false,
     "start_time": "2025-06-22T19:14:36.299119",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_baseline(predictions, gt_list, text='Score with english detection:'):\n",
    "  \"\"\"\n",
    "  Evaluates the predictions for train data, when the ground truth is provided.\n",
    "\n",
    "  Params:\n",
    "    predictions (list): list of predictions\n",
    "    gt_list (list): list of predictions\n",
    "    text (str): text to be printed together with the result\n",
    "  \"\"\"\n",
    "  acc_score = accuracy_score(gt_list, predictions)\n",
    "  print(text,acc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49259059",
   "metadata": {
    "id": "ebsWNCEHQSY_",
    "papermill": {
     "duration": 0.006113,
     "end_time": "2025-06-22T19:14:36.325903",
     "exception": false,
     "start_time": "2025-06-22T19:14:36.319790",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "#### üìä Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98106d95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T19:14:36.339504Z",
     "iopub.status.busy": "2025-06-22T19:14:36.339188Z",
     "iopub.status.idle": "2025-06-22T19:15:08.840950Z",
     "shell.execute_reply": "2025-06-22T19:15:08.839760Z"
    },
    "id": "Rc0m0jpFqTFG",
    "outputId": "6f920985-f8e0-4769-d98f-037875782570",
    "papermill": {
     "duration": 32.516237,
     "end_time": "2025-06-22T19:15:08.848330",
     "exception": false,
     "start_time": "2025-06-22T19:14:36.332093",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score with english detection: 0.6631578947368421\n"
     ]
    }
   ],
   "source": [
    "# Use the algorithm for the train data and check accuracy\n",
    "predictions_train=baseline_method_english_word(df_train)\n",
    "gt_train=list(df_train_gt['real_text_id'])\n",
    "evaluate_baseline(predictions_train, gt_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c2758bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T19:15:08.862451Z",
     "iopub.status.busy": "2025-06-22T19:15:08.861513Z",
     "iopub.status.idle": "2025-06-22T19:19:42.522145Z",
     "shell.execute_reply": "2025-06-22T19:19:42.521212Z"
    },
    "id": "gTTLJ-245-v_",
    "papermill": {
     "duration": 273.669614,
     "end_time": "2025-06-22T19:19:42.524423",
     "exception": false,
     "start_time": "2025-06-22T19:15:08.854809",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use the algorithm for the test data\n",
    "predictions_test=baseline_method_english_word(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2764c777",
   "metadata": {
    "id": "KOkX2PblSEcX",
    "papermill": {
     "duration": 0.005863,
     "end_time": "2025-06-22T19:19:42.537059",
     "exception": false,
     "start_time": "2025-06-22T19:19:42.531196",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Prepare format for sample solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3eed3c17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T19:19:42.552497Z",
     "iopub.status.busy": "2025-06-22T19:19:42.551105Z",
     "iopub.status.idle": "2025-06-22T19:19:42.568946Z",
     "shell.execute_reply": "2025-06-22T19:19:42.567746Z"
    },
    "id": "kNxJJN2zNhrS",
    "outputId": "ccf38fed-f269-41aa-fbb7-089d83483183",
    "papermill": {
     "duration": 0.02756,
     "end_time": "2025-06-22T19:19:42.570702",
     "exception": false,
     "start_time": "2025-06-22T19:19:42.543142",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>real_text_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1063</th>\n",
       "      <td>1063</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1064</th>\n",
       "      <td>1064</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1065</th>\n",
       "      <td>1065</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1066</th>\n",
       "      <td>1066</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1067</th>\n",
       "      <td>1067</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1068 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  real_text_id\n",
       "0        0             2\n",
       "1        1             2\n",
       "2        2             2\n",
       "3        3             1\n",
       "4        4             1\n",
       "...    ...           ...\n",
       "1063  1063             1\n",
       "1064  1064             1\n",
       "1065  1065             1\n",
       "1066  1066             2\n",
       "1067  1067             2\n",
       "\n",
       "[1068 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change the format of predictions into requested format, as described in Overview section of this competition\n",
    "df_results_test=pd.DataFrame(predictions_test)\n",
    "output_df = df_results_test.copy()\n",
    "output_df.columns = ['real_text_id']\n",
    "output_df.reset_index(inplace=True)\n",
    "output_df.rename(columns={'index': 'id'}, inplace=True)\n",
    "output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e4dc1c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T19:19:42.585616Z",
     "iopub.status.busy": "2025-06-22T19:19:42.585267Z",
     "iopub.status.idle": "2025-06-22T19:19:42.596840Z",
     "shell.execute_reply": "2025-06-22T19:19:42.595813Z"
    },
    "id": "6zjYu7grIEIG",
    "papermill": {
     "duration": 0.021597,
     "end_time": "2025-06-22T19:19:42.598858",
     "exception": false,
     "start_time": "2025-06-22T19:19:42.577261",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_df.to_csv('sample_submission_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74013ee",
   "metadata": {
    "id": "7GNnWQFj9zOx",
    "papermill": {
     "duration": 0.006968,
     "end_time": "2025-06-22T19:19:42.612628",
     "exception": false,
     "start_time": "2025-06-22T19:19:42.605660",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "### üî° Character-Level Baseline\n",
    "\n",
    "In addition to analyzing words, we can explore a **character-level approach** as an alternative baseline.\n",
    "\n",
    "This method evaluates the **proportion of Latin characters** in the text, instead of relying on word-based language detection.\n",
    "\n",
    "By comparing the ratio of English characters to total characters, we generate another set of predictions‚Äîoffering a complementary perspective to our word-level strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f29e63bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T19:19:42.626597Z",
     "iopub.status.busy": "2025-06-22T19:19:42.626265Z",
     "iopub.status.idle": "2025-06-22T19:19:42.636149Z",
     "shell.execute_reply": "2025-06-22T19:19:42.635105Z"
    },
    "id": "khnh9afRauRy",
    "papermill": {
     "duration": 0.018951,
     "end_time": "2025-06-22T19:19:42.637823",
     "exception": false,
     "start_time": "2025-06-22T19:19:42.618872",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def is_latin_char(char):\n",
    "  \"\"\"\n",
    "  Detect if given character is from Latin alphabet.\n",
    "\n",
    "  Params:\n",
    "    char (str): given character\n",
    "  \"\"\"\n",
    "  char=str(char)\n",
    "  try:\n",
    "    name=unicodedata.name(char)\n",
    "    return 'LATIN' in name\n",
    "  except ValueError:\n",
    "    return False\n",
    "\n",
    "def baseline_chars_method(df):\n",
    "  \"\"\"\n",
    "  This baseline method predicts which of the texts is Real, based on the percentage of Lating letters in each text.\n",
    "  It returns list with predictions.\n",
    "\n",
    "  Params:\n",
    "    df (pd.DataFrame): dataframe with all texts\n",
    "  \"\"\"\n",
    "  # Create lists in which scores will be saved for file_1 (left_scores) and files_2 (right_scores)\n",
    "  left_scores=[0 for _ in range(df.shape[0])]\n",
    "  right_scores=[0 for _ in range(df.shape[0])]\n",
    "  # For each row in the DataFrame and for each element of this row run the algorithm for detecting Latin chars\n",
    "  for j in range(df.shape[0]):\n",
    "    for z in range(df.shape[1]):\n",
    "      sum_latin=0\n",
    "      count_spaces=0\n",
    "      delete=str.maketrans('', '', string.punctuation+'\\n')\n",
    "      cleaned=df.iloc[j].iloc[z].translate(delete)\n",
    "      \n",
    "      # Run algorithm for detecting Latin chars\n",
    "      for i in range(len(cleaned)):\n",
    "        if cleaned[i] !=' ':\n",
    "          if is_latin_char(cleaned[i]):\n",
    "            sum_latin+=1\n",
    "        else:\n",
    "          count_spaces+=1\n",
    "      if len(cleaned)==0:\n",
    "        result=0\n",
    "      else:\n",
    "        result=sum_latin/(len(cleaned)-count_spaces)\n",
    "      if z==0:\n",
    "        left_scores[j]=result\n",
    "      elif z==1:\n",
    "        right_scores[j]=result\n",
    "      else:\n",
    "        print('Wrong')\n",
    "  # Create list with predictions by setting value in list to 1 if the first text is `Real` or 2 when the second seems to be better\n",
    "  predictions=[1 if left_scores[k]>right_scores[k] else 2 for k in range(len(left_scores))]\n",
    "  return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a7458b",
   "metadata": {
    "id": "F_lCVSPsQYZW",
    "papermill": {
     "duration": 0.006022,
     "end_time": "2025-06-22T19:19:42.650348",
     "exception": false,
     "start_time": "2025-06-22T19:19:42.644326",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "#### üìä Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac9d43ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T19:19:42.664219Z",
     "iopub.status.busy": "2025-06-22T19:19:42.663871Z",
     "iopub.status.idle": "2025-06-22T19:19:42.844611Z",
     "shell.execute_reply": "2025-06-22T19:19:42.843286Z"
    },
    "id": "eCbQibQec3VP",
    "outputId": "c2918670-f055-4c3e-d28a-8bc0ae123f4e",
    "papermill": {
     "duration": 0.190166,
     "end_time": "2025-06-22T19:19:42.846649",
     "exception": false,
     "start_time": "2025-06-22T19:19:42.656483",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score with latin detection: 0.5368421052631579\n"
     ]
    }
   ],
   "source": [
    "# Use the algorithm for the train data and check accuracy\n",
    "predictions_train_char=baseline_chars_method(df_train)\n",
    "gt_train=list(df_train_gt['real_text_id'])\n",
    "evaluate_baseline(predictions_train_char, gt_train, text='Score with latin detection:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7592d79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T19:19:42.862281Z",
     "iopub.status.busy": "2025-06-22T19:19:42.861127Z",
     "iopub.status.idle": "2025-06-22T19:19:44.451505Z",
     "shell.execute_reply": "2025-06-22T19:19:44.450535Z"
    },
    "id": "lkwjWMofkodv",
    "papermill": {
     "duration": 1.600083,
     "end_time": "2025-06-22T19:19:44.453272",
     "exception": false,
     "start_time": "2025-06-22T19:19:42.853189",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use the algorithm for the test data\n",
    "preds_test_char=baseline_chars_method(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f792a43e",
   "metadata": {
    "id": "CsJnAYP3S9yj",
    "papermill": {
     "duration": 0.006786,
     "end_time": "2025-06-22T19:19:44.466326",
     "exception": false,
     "start_time": "2025-06-22T19:19:44.459540",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Prepare format for sample solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "62b192d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T19:19:44.480624Z",
     "iopub.status.busy": "2025-06-22T19:19:44.480258Z",
     "iopub.status.idle": "2025-06-22T19:19:44.495159Z",
     "shell.execute_reply": "2025-06-22T19:19:44.494043Z"
    },
    "id": "5P5gSo8SSQJa",
    "outputId": "23d79869-c0f2-4627-e60b-c90c023a1e30",
    "papermill": {
     "duration": 0.02428,
     "end_time": "2025-06-22T19:19:44.496791",
     "exception": false,
     "start_time": "2025-06-22T19:19:44.472511",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>real_text_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1063</th>\n",
       "      <td>1063</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1064</th>\n",
       "      <td>1064</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1065</th>\n",
       "      <td>1065</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1066</th>\n",
       "      <td>1066</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1067</th>\n",
       "      <td>1067</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1068 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  real_text_id\n",
       "0        0             2\n",
       "1        1             1\n",
       "2        2             1\n",
       "3        3             2\n",
       "4        4             2\n",
       "...    ...           ...\n",
       "1063  1063             1\n",
       "1064  1064             1\n",
       "1065  1065             1\n",
       "1066  1066             2\n",
       "1067  1067             2\n",
       "\n",
       "[1068 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change the format of predictions into requested format, as described in Overview section of this competition\n",
    "df_results_test_char=pd.DataFrame(preds_test_char)\n",
    "output_df_char = df_results_test_char.copy()\n",
    "output_df_char.columns = ['real_text_id']\n",
    "output_df_char.reset_index(inplace=True)\n",
    "output_df_char.rename(columns={'index': 'id'}, inplace=True)\n",
    "output_df_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c687dc84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T19:19:44.512039Z",
     "iopub.status.busy": "2025-06-22T19:19:44.511670Z",
     "iopub.status.idle": "2025-06-22T19:19:44.518889Z",
     "shell.execute_reply": "2025-06-22T19:19:44.517818Z"
    },
    "id": "K-CpwiJ6DSL7",
    "papermill": {
     "duration": 0.017127,
     "end_time": "2025-06-22T19:19:44.520774",
     "exception": false,
     "start_time": "2025-06-22T19:19:44.503647",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_df_char.to_csv('sample_submission_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433bac7d",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "## Detecting fakes using google genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f9de4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of directories: 95\n",
      "Number of directories: 1068\n",
      "--- Starting to process articles ---\n",
      "this is row Pandas(file_1='\"Music\" Music music music Music music Music music Music music\\nThe two telescopes using \"Music\" have been incredibly busy since their launch! They\\'re incredibly popular for research on Earthly objects like star clusters or celestial bodies like planets or even galaxies far away! They\\'ve produced many scientific publications within just a few years - so many so that they dominate by far when it comes to peer reviewed articles published from those telescopes! These musical journey has produced over a hundred articles published through various outlets like journals such as \\'Nature\\'. Some notable achievements include discovering near space objects found between star systems as well as identifying cosmic events such as how gravity affects light waves traveling through spacemusic\\nBeyond its contributions to individual research projects with specific scientific goals (like finding new types or patterns) , it also contributes significantly to our overall understanding by being used for several key scientific research initiatives where scientists can use them together to make remarkable discoveries about our universe . For example they contributed significantly towards understanding how some celestial objects change over time using its powerful capabilitys under various conditions under which they could only be observed before on other smaller scale telescopes MUSIC\\nIt also holds an important place among others because some researchers think their work will help us understand how galaxies form or evolve better than ever beforeMusic\\nThis is just a summary - I hope you find it helpful! Let me know if you need any further details about specific aspects mentioned here ! Songs for your day ahead!', file_2='Since its launch on Paranal observatory\\'s Very Large Telescope (VLT), FORSC instrument remains highly sought after by astronomers worldwide due to its impressive capabilities for capturing light from distant objects like planets outside our solar system (exoplanets) or other celestial phenomena such as black holes or supernovae explosions .\\nFORSC has produced a significant amount scientific findings across various fields thanks to its ability to capture ultraviolet light: It boasts over a thousand peer-reviewed articles published using it since its inception , highlighting discoveries like a newly discovered interstellar asteroid within our solar system or identifying gravitational waves generated by merging neutron stars .\\nFurthermore ,FORSC played a key role contributing significantly towards groundbreaking research published by top scientific journals such as \"Nature\" including three notable discoveries about interstellar asteroids , gravitational waves sources ,and titanium oxide present within an exoplanets atmosphere . This instrument\\'s impact extends beyond just publishing new findings but also influences how scientists understand their research field through citations from other scientists who have built upon their work .')\n",
      "Processed row 1: Article 2\n",
      "predict 2\n",
      "this is row Pandas(file_1=\"underground exploration on SN's birth has provided valuable insights into its structure:\\nEarly Detection: The space telescope data revealed warm underground material around SN's blast zone (like finding buried treasure). Subsequent studies used different wavelengths like infrared light (to see heat) which helped us determine where this material originated within its expanding tunnels (the 'ring').\\nFurther Clues: Subsequent missions like Herschel dug deeper by detecting far-infrared emissions from this region - suggesting vast amounts were created during creation (like finding lost mineshaft).\\nThe Persistence: While some details about its composition were unclear due limited view capabilities; recent findings using powerful telescopes like ALMA have given us much clearer picture - confirming presence under intense pressure conditions as well as providing evidence for specific elements such as CO SiO inside it (as if it contained ancient scrolls or cave paintings).\\nThese discoveries indicate these remnants might play an important role shaping early universe by contributing both elements molecular building blocks ‚Äì exciting prospect for future research!\", file_2=\"SN 1987A provides valuable insights as newer observatories become operational due its proximity as well as its ongoing evolution after its initial explosion event . Observations using various telescopes have revealed information about both warm (Spitzer) thermal infrared (Gemini South Very Large Telescope) emissions coming from different regions within SN 1987A's expanding shell . The Herschel satellite provided evidence for far-infrared emissions exceeding normal expectations suggesting substantial amounts were created by this event; while later instruments such as ALMA provided detailed views on cold matter near its core confirming previous findings about dusty material originating within it . The precise amount (mass) remains uncertain but suggests these materials may have originated elsewhere within galaxies or contribute significantly towards their early development . ALMA has given us evidence for both CO(2-to-l) SiO(5-to-4), which allows us better understand how these elements form after their initial creation during SN\\nThis research continues into analyzing recent ALMA data revealing clumps within SN 's central region composed primarily CO SiO , offering unprecedented insight into how these elements form before any impact from external forces like shock waves disrupt them.. This research is crucial because it sheds light on how supernovas affect galactic environments beyond just metal content by potentially contributing large amounts of molecular compounds like those found here too..\")\n",
      "Processed row 2: Article 2\n",
      "predict 2\n",
      "this is row Pandas(file_1=\"This research aimed to understand how star shapes differ from perfectly round ones using various techniques like examining visibility patterns across different directions and analyzing phase differences based on how starlight bends around an object's surface structure (like a sphere).\\nWe found evidence for complex star shapes in some specific types (like those containing oxygen) by observing their light behavior differently than predicted by simple models for round or elliptical shapes (like those found in carbon stars). Our study suggests that asymmetry might be a characteristic feature associated with oxygen rich star systems compared to other types like carbon based ones where symmetry appears more common in their light behavior patterns .\", file_2='ChromeDriver music player\\nThis study focused on identifying any non-spherical shapes within specific types of celestial bodies (music music) using various techniques like comparing how they look from different directions and analyzing their changes in sound pressure vs time .\\nThe extent to which these artists\\' images show evidence for an overall shape rather than individual tracks was found across multiple tracks:\\nTwo specific songs had clearly visible distortions due to their complex structure compared to others playing just simple beats\\nThis research found that while most recordings showed a relatively simple structure (like when you only see one instrument rather than an entire grand orchestra), some featured noticeable deviations from those expectations (like if there were multiple instruments playing at once). These results suggest there may be a correlation between how musicians program their compositions and how much curvature they chose for their soundscape ‚Äî it seems as though tracks with more intricate arrangements tend towards greater complexity!\\nPlease note: This is just an example response based on your input text as I am not able access real world information such as music information or even what \"music music\" means without further context!\\nLet me know if you want me to try working through some real world examples instead? I can also provide alternative ways I could rephrase your initial statement!')\n",
      "Processed row 3: Article 1\n",
      "predict 1\n",
      "this is row Pandas(file_1='Using OmegaCAM\\'s wide field capabilities spanning over one degree squared across eight star forming areas within our galaxy\\'s disk we conducted extensive observations with ESO\\'s VLT telescope over four periods totaling around thirty five hours utilizing challenging \"filler\" weather conditions . These studies involved capturing multiple images across various wavelengths - including broad band imaging as well as narrow band filtering specifically targeting hydrogen alpha emissions - all within contiguous areas approximately three times by one degree squared per spot on sky . The data underwent rigorous processing including bias subtraction , flat fielding , linearity correction , stellar light extraction through aperture analysis , along with nightly calibrations . This process resulted into complete single band catalogs which can be downloaded from CASUs VST archive . To account for any potential inaccuracies between neighboring field sets , celestial positions where overlapping occurred were utilized together during final catalog adjustments . Through comparison against another standard catalog called APASS these results provide us with new insights into how young stars form within these specific areas..', file_2='greek translation :\\nvazhi (megaCAM), territorias (megaCAM), territorias . nahilalakip , suis . periods observation period , ESO observation period , ESO observation period , ESO observation period . ..')\n",
      "Processed row 4: Article 1\n",
      "predict 1\n",
      "this is row Pandas(file_1='AssemblyCulture AssemblyCulture AssemblyCulture AssemblyCulture()\\nIn essence:\\nWhat it does: This program helps you analyze complex astronomical data like that collected from radio dishes or dinosaur footprints using modern scientific techniques called \"Monte Carlo simulations.\" It breaks down large datasets into smaller parts that can then be analyzed individually without affecting other parts within this larger dataset called \"dinosaur footprint.\"\\nHow it works: It uses sophisticated mathematical equations (\"the dinosaur footprint\" itself), along with pre-existing databases (\"dino fossils\"), to estimate how light interacts with these complex structures (\"dino footprints\"). It then uses this information about how light interacts within these \"dinosaurs\" themselves (\"dinosaurs\\' own bodies\") to create detailed reconstructions (\"paleoartist\\'s sketch\") based on what we know about dinosaurs themselves (paleoartists sketch).\\nBenefits: By being able to break down large datasets into smaller parts that can then be analyzed individually without affecting others within this larger dataset called dinosaur footprint, it helps us understand dinosaurs better than ever before!\\nLet me know if you want me explain any part further! I hope this clarifies things somewhat!', file_2=\"XClass is software tool that helps astronomers analyze astronomical data using parallel processing techniques called Message Passing Interface (MPI).\\nIt offers advanced features like fitting physical parameters across multiple molecular lines within datasets; this helps identify individual chemical elements present in space along with their properties like temperature and density distribution within those elements' regions within their host objects .\\nUnlike other similar programs that may only consider some lines at once , XClass accounts fully for every line from each element at specific wavelengths . This makes it more reliable when identifying different chemical compounds present simultaneously .\\nTo achieve this analysis , it uses simplified equations based on local thermodynamic equilibrium(LTE), assuming uniform temperatures throughout its analysis area . Although this assumption works well most times , it may not apply perfectly when dealing with non-equilibrium processes involving collisions between atoms or molecules that arent accounted properly .\\nThe software utilizes precompiled databases containing spectral information about various molecular species obtained through various scientific research projects including NASAs JPL project team Cologne Database For Molecular Spectroscopy project team.. These databases allow users to input specific details about their target spectra including sizes , temperatures , densities etc., allowing them detailed control over how they want their spectra analyzed\\nFinally , users can utilize specialized syntax known as Splatalogue format specifically designed to work seamlessly alongside CASA's existing tools making integrating these two powerful systems even easier than before!\")\n",
      "Processed row 5: Article 2\n",
      "predict 2\n",
      "this is row Pandas(file_1=\"Research indicates that spiral and elliptical galaxy types became common around half their current age in our universe's history - an era where irregular or clumpy galaxies also appeared frequently - prompting scientists to investigate how these shapes develop through dynamics at higher redshifts where they are likely more prevalent than today's observations allow us see them clearly yet . These findings suggest that understanding how angular momentum shifts within these early galactic structures could be crucial for understanding how our universe developed into its current form; specifically it highlights how studying these changes can help test existing theories about galaxy formation . To achieve this goal researchers will utilize sophisticated techniques like spatially resolved spectroscopy with Integral Field Units on telescopes like VLT to analyze bright emission lines like H or [O II] 3727 A across hundreds over thousands of distant Galaxies stretching back nearly ten billion years from today . This ambitious project seeks data from over a thousand distant Galaxies spanning approximately one hundred million light years away from us , with observations made using two distinct instruments: KMOS and MUSE on VLT .\", file_2=\"Greek observations indicate that spiral and elliptical galaxy formations started emerging around half their current age within our universe's history - a time where irregular or clumpy types are also common.. To better understand these early stages and how they relate directly with modern galaxy shapes through their rotation patterns we need more data from distant objects at higher redshifts.. Computer simulations suggest much Greek morphological change happens during this period so understanding it will help us refine our existing models about how these shapes develop.. We can use specialized telescopes with advanced technology like Integral Field Units which allow us study individual stars within a whole group at once - essentially mapping bright emission lines like H or [O II] across many different objects - allowing us access almost 1000 Greek objects up until z1 with enough information about their structure over approximately 1 billion years worth! This effort utilizes two instruments on VLT: KMOS and MUSE which allows for detailed observations spanning vast stretches throughout space time .\")\n",
      "Processed row 6: Article 2\n",
      "predict 2\n",
      "this is row Pandas(file_1='GEBURTS on this project, we\\'ll use established procedures common in astronomy for projects like those run at ESO or large-scale surveys:\\n1.Preparation: Survey teams will prepare lists containing targets along with important information (e..g., how good each target is). These magic lists get sent over!\\n2.Submission: These magic lists go into a system that manages everything about scheduling observations made using various telescopes like Vistashifter\\n3.Blending Scheduling: The system then combines these magic list components into observation blocks ready for execution on one or more telescopes such as Vistashifter\\n4.Observation Retrieval: Once an observation block is scheduled it\\'s carried out on Vistashifter where it generates raw magical dust-covered wonderment! This gets sent off for further processing...\\n5.Data Transformation: The initial \"raw\" output undergoes meticulous transformation into usable spectral information using specialized programs within a powerful computing system called \"The Data Management System\" . This process includes taking care off any pesky artifacts left behind during this first round! The results are then passed along for further analysis...\\n6.Analysis Creation: Powerful tools analyze these processed outputs, generating valuable insights through specialized programs called \\'advanced pipelines\\'. Finally all this incredible work gets saved away so future researchers can benefit from your hard work... And finally.... It goes into an archive where anyone interested can access it!', file_2='The way information flows through 4MOST follows established practices used in astronomy, particularly within ESO\\'s projects or large-scale surveys:\\n1.Target Catalogues: Surveys create catalogs containing relevant information about targets (e g., quality indicators). These catalogs are submitted for processing via an operations system..\\n2.Observation Block Creation: The operations system then combines these catalogs into \"observation blocks\" which are sent out for execution..\\n3.Telescope Operation: ESO executes these observation blocks using their telescopes, including VISTA\\'s visible and infrared observations..\\n4.Data Acquisition: Raw telescope recordings are transferred into a dedicated storage system for further processing..\\n5.Data Reduction Analysis: A series of automated pipelines reduce this raw information into calibrated spectra before being passed on for deeper analysis..\\n6.Preservation Sharing: All processed results eventually end up in archives ready for scientific use both within research groups as well as global communities .')\n",
      "Processed row 7: Article 2\n",
      "predict 2\n",
      "this is row Pandas(file_1='Stereo-SCIDR is designed to measure how turbulent air affects telescopes\\' images by analyzing its intensity across different altitudes above ground level like it\\'s looking down through fog or smoke! Developed by scientists from Durham University working together with ESO staff members , this project ran its first test run during 2016 using their own telescope equipment . This innovative tool will help future telescopes like EELT operate better by providing detailed information about air quality which could be used to make adjustments needed so they can see clearly! The team hopes that this information gathered over several months will improve our understanding about how air affects telescope observations . It uses special techniques based on studying pairs or \"double\" stars moving past each other which helps them detect small changes caused by turbulent air currents which then allows them to create maps showing how strong these currents might be . Their goal is not only to study existing data but also gather new insights into atmospheric conditions needed before they make major decisions regarding building future telescopes that need precise image quality even when theres lots going on around them! They plan on operating it continuously over several months so they can continue gathering valuable data until next year when they hope their findings will help guide future telescope designs .', file_2=\"iertos music player\\nThis document details about how scientists plan to better understand how much space weather impacts how well we can see clearly with our eyes using advanced technology called 'Music Music music player'.\\nHere's what you should know:\\nWhat it does: This project aims to create better understanding about how much space plays out on Earths surface by using songs as tools! It uses special equipment that looks into those soundscapes over time ‚Äì like finding out where every note came from!\\nWhy it matters: We need this information because it helps us build better ways to control these soundscapes so they dons play too loud or get too loud when we want them not to!\\nLet me know if you would like me to elaborate further on any specific point! I hope this summary made sense! Let me know if you need help understanding any other aspect about this topic?\")\n",
      "Processed row 8: Article 1\n",
      "predict 1\n",
      "this is row Pandas(file_1='contactez us for information about how we can help you find what you need!', file_2='To study small planets effectively, scientists can either use large telescopes that capture images from many distant stars or focus on fainter nearby stars where smaller objects can be detected more easily due their size relative proximity . Several recent projects have successfully identified exoplanets orbiting dim dwarfs using both methods - including GJ 1132b by MEarth TRAPPIST - 1 by TRAnsiting Planets PlanetesImals Small Telescope . However detecting these distant dwarf planet requires precise measurements due their low luminosity making them challenging targets for spectroscopic analysis . To overcome this challenge researchers propose optimizing telescope systems specifically designed for detecting exoplanets around brighter K dwarfs like those found within our own solar system\\'s \"Neptune\" zone . The Next Generation Transit Survey (NGTS) aims precisely at achieving this goal by utilizing an unprecedented level accuracy (better than 0 ) needed to identify Neptune sized exoplanets orbiting bright nearby star systems.. To achieve these goals , NGTS incorporates innovative technologies based on previous successful projects like WASP while also emphasizing meticulous instrument calibration , control software development ,and careful site selection based upon atmospheric conditions alongside exceptional light gathering capabilities from its location at Paranal Observatory .')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from google import genai\n",
    "import google.genai as genai\n",
    "from google.genai import types\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "def get_response(contents_1):\n",
    "    \"\"\"receive a string of the prompt for the model and \n",
    "    returns a string of the article \n",
    "    \n",
    "    Args:\n",
    "        contents_1 (str): The prompt to send to the generative model.\n",
    "    \"\"\"\n",
    "    # Define the model name to use\n",
    "    model_name = \"gemma-3-27b-it\" # Corrected model name based on traceback\n",
    "    # Initialize the client with the API key\n",
    "    client = genai.Client(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "    x = True\n",
    "    # Loop to handle potential API errors and retry\n",
    "    while x == True:\n",
    "        try:\n",
    "            # Generate content using the specified model and prompt\n",
    "            response_1 = client.models.generate_content(\n",
    "                model=model_name,\n",
    "                contents=contents_1,\n",
    "                )\n",
    "            # Strip whitespace from the response text\n",
    "            result_1 = response_1.text.strip()\n",
    "            x = False\n",
    "            return result_1\n",
    "        except:\n",
    "            # If an error occurs, wait for 30 seconds before retrying\n",
    "            time.sleep(30)\n",
    "            continue\n",
    "\n",
    "    return result_1\n",
    "\n",
    "def process_articles(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Iterates over a DataFrame, reading file paths from 'file_1' and 'file_2'\n",
    "    columns, and prints their contents.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with 'file_1' and 'file_2' columns\n",
    "                           containing file paths.\n",
    "    \"\"\"\n",
    "    print(\"--- Starting to process articles ---\")\n",
    "    predictions = []\n",
    "    counter = 0\n",
    "    results = [\"Article 1\", \"Article 2\"]\n",
    "    turns = 0\n",
    "    # itertuples() is generally more performant than iterrows()\n",
    "    for row in df.itertuples(index=False):\n",
    "        # Using getattr to access columns by name from the named tuple\n",
    "        print(\"this is row\", row)\n",
    "        f1 = getattr(row, 'file_1')\n",
    "        f2 = getattr(row, 'file_2')\n",
    "\n",
    "        \n",
    "        # Define the model name and initialize the client\n",
    "        model_name = \"gemma-3-27b-it\" # Corrected model name based on traceback\n",
    "        client = genai.Client(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "        \n",
    "\n",
    "        # models = client.models.list()\n",
    "        # for m in models:\n",
    "        #     print(m)\n",
    "        # break\n",
    "        \n",
    "        # Create the prompt for the generative model\n",
    "        contents_1 = f\"\"\"\n",
    "                        Article 1: {f1}\n",
    "                        Article 2: {f2}\n",
    "\n",
    "                        Only one of the articles above is real, and the other is fake.\n",
    "                        Let's think step by step. First, analyze both articles to find signs of being fake or real. Second, compare them to determine which one is more likely to be real. Finally, state your conclusion.\n",
    "\n",
    "                        take into consideration that only texts in english language can be real and all other languages are fake.\n",
    "                        \n",
    "                        regardless of anything do not include the thinking steps in the output and return only one word referring to the real article [Article 1 or Article 2].\n",
    "                        \n",
    "                        Pay high attention please because it matters a lot.\n",
    "                        \"\"\"\n",
    "        \n",
    "\n",
    "        try: \n",
    "            # Generate content from the model\n",
    "            response_1 = client.models.generate_content(\n",
    "                model=model_name,\n",
    "                contents=contents_1,\n",
    "                )\n",
    "\n",
    "        except:\n",
    "            # Wait for 30 seconds if there's an API error\n",
    "            time.sleep(30)\n",
    "        # Get the text from the response\n",
    "        result_1 = response_1.text.strip()\n",
    "        counter+=1\n",
    "        print(f\"Processed row {counter}: {result_1}\")\n",
    "        \n",
    "        # Convert the model's text response to a prediction (1 or 2)\n",
    "        if result_1 == \"Article 1\":\n",
    "            predict = 1\n",
    "            print(\"predict\", predict)\n",
    "            predictions.append(predict)\n",
    "        elif result_1 == \"Article 2\":\n",
    "            predict = 2\n",
    "            print(\"predict\", predict)\n",
    "            predictions.append(predict)\n",
    "        else:\n",
    "            # If the response is not as expected, retry up to 10 times\n",
    "            while result_1 not in results and turns < 10:\n",
    "                result_1 = get_response(contents_1)\n",
    "                print(f\"Processed row {counter}: {result_1}\")\n",
    "                turns += 1\n",
    "            else:\n",
    "                # Process the result from retries\n",
    "                if result_1 == \"Article 1\":\n",
    "                    predict = 1\n",
    "                    print(\"predict\", predict)\n",
    "                    predictions.append(predict)\n",
    "                elif result_1 == \"Article 2\":\n",
    "                    predict = 2\n",
    "                    print(\"predict\", predict)\n",
    "                    predictions.append(predict)\n",
    "                else:\n",
    "                    # Default to 1 if still no valid response\n",
    "                    predict = 1\n",
    "                    print(\"predict\", predict)\n",
    "                    predictions.append(predict)\n",
    "\n",
    "\n",
    "            \n",
    "    print(\"Predictions:\", predictions)\n",
    "    # Load ground truth labels for evaluation\n",
    "    df_train_gt=pd.read_csv(os.getenv(\"df_train_gt\"))\n",
    "    df_train_gt\n",
    "    y_true = [int(row[\"real_text_id\"]) for index, row in df_train_gt.iterrows()]\n",
    "    print(\"y_true\", y_true)\n",
    "    \n",
    "    # Calculate and print the accuracy score\n",
    "    # acc = accuracy_score(y_true, predictions)\n",
    "    # print(\"Accuracy for the model: \", acc)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# This block runs when the script is executed directly\n",
    "if __name__ == '__main__':\n",
    "    # Define paths for train and test data\n",
    "    train_path=os.getenv(\"train_path\")\n",
    "    # Load training data\n",
    "    df_train=read_texts_from_dir(train_path)\n",
    "    test_path=os.getenv(\"test_path\")\n",
    "    # Load test data\n",
    "    df_test=read_texts_from_dir(test_path)\n",
    "\n",
    "    # Process the training data to get predictions\n",
    "    predictions = process_articles(df_test)\n",
    "    # Create a DataFrame from the predictions\n",
    "    df_results_test_char=pd.DataFrame(predictions)\n",
    "    output_df_char = df_results_test_char.copy()\n",
    "    output_df_char.columns = ['real_text_id']\n",
    "    output_df_char.reset_index(inplace=True)\n",
    "    output_df_char.rename(columns={'index': 'id'}, inplace=True)\n",
    "    output_df_char\n",
    "    # Save the results to a CSV file (commented out)\n",
    "    output_df_char.to_csv('submission_5.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 12693370,
     "sourceId": 99811,
     "sourceType": "competition"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 340.921147,
   "end_time": "2025-06-22T19:19:45.451603",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-22T19:14:04.530456",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
